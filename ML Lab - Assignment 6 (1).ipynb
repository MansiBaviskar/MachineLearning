{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8XfmRKMviRq"
   },
   "source": [
    "ML Practical 6\n",
    "\n",
    "Name - Mansi Mohan Baviskar\n",
    "\n",
    "Roll No.- 42505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4x_wD13LrKLE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the Maze\n",
    "class Maze:\n",
    "    def __init__(self):\n",
    "        self.grid = np.array([[0, 0, 0, 0, 0],\n",
    "                               [0, 1, 1, 1, 0],\n",
    "                               [0, 1, 0, 0, 0],\n",
    "                               [0, 1, 1, 1, 0],\n",
    "                               [0, 0, 0, 0, 2]])\n",
    "        self.start_state = (0, 0)  # Starting position\n",
    "        self.goal_state = (4, 4)    # Goal position\n",
    "        self.state = self.start_state\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "\n",
    "        if action == 0:  # Up\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 1:  # Down\n",
    "            x = min(4, x + 1)\n",
    "        elif action == 2:  # Left\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 3:  # Right\n",
    "            y = min(4, y + 1)\n",
    "\n",
    "        if self.grid[x, y] == 1:  # If hitting a wall\n",
    "            return self.state, -1, False  # Return current state, penalty, and not done\n",
    "\n",
    "        self.state = (x, y)\n",
    "\n",
    "        if self.state == self.goal_state:  # Reached the goal\n",
    "            return self.state, 10, True  # Return goal state, reward, and done\n",
    "        else:\n",
    "            return self.state, -0.1, False  # Penalty for each step taken\n",
    "\n",
    "    def render(self):\n",
    "        maze_copy = self.grid.copy()\n",
    "        x, y = self.state\n",
    "        maze_copy[x, y] = 3  # Mark the agent's position\n",
    "        print(maze_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N9WlkezKrfxW"
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, maze):\n",
    "        self.maze = maze\n",
    "        self.q_table = np.zeros((5, 5, 4))  # 5x5 grid and 4 actions\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.min_epsilon = 0.1\n",
    "        self.num_episodes = 1000\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.randint(0, 3)  # Explore: choose random action\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state[0], state[1]])  # Exploit: choose best action\n",
    "\n",
    "    def learn(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            state = self.maze.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.maze.step(action)\n",
    "\n",
    "                # Update Q-table using the Q-learning formula\n",
    "                best_next_action = np.argmax(self.q_table[next_state[0], next_state[1]])\n",
    "                td_target = reward + self.discount_factor * self.q_table[next_state[0], next_state[1], best_next_action]\n",
    "                td_delta = td_target - self.q_table[state[0], state[1], action]\n",
    "                self.q_table[state[0], state[1], action] += self.learning_rate * td_delta\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Decay epsilon\n",
    "            if self.epsilon > self.min_epsilon:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def print_q_table(self):\n",
    "        print(self.q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 887,
     "status": "ok",
     "timestamp": 1728333102704,
     "user": {
      "displayName": "2209_SURAJ CHAUDHARI",
      "userId": "02488172901437088190"
     },
     "user_tz": -330
    },
    "id": "RXrbpjm1rhq4",
    "outputId": "0de0d00b-31cc-433e-b5f8-aa1c9cd00485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 3.52245392e+00  4.26126590e+00  3.59136049e+00  8.16229267e-01]\n",
      "  [-2.37426713e-01 -1.03542103e+00  7.35023142e-01  1.61444984e+00]\n",
      "  [-8.78206981e-02 -7.22994850e-01 -1.43662834e-01  2.78449792e+00]\n",
      "  [ 5.69047507e-02 -7.31362211e-01  5.58120291e-02  4.20725462e+00]\n",
      "  [ 5.23715267e-01  5.70416737e+00  6.82754864e-01  1.41464406e+00]]\n",
      "\n",
      " [[ 3.26639895e+00  4.84585100e+00  4.09408210e+00  3.16000727e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 1.02001126e+00  7.30873569e+00  7.77316089e-01  2.35788691e+00]]\n",
      "\n",
      " [[ 3.93498735e+00  5.49539000e+00  4.73585078e+00  3.82113893e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [-4.86843102e-01 -4.84434437e-01 -3.72241337e-01  6.24460058e-01]\n",
      "  [ 9.67697747e-02  5.26060649e-02  7.52709672e-03  3.29947427e+00]\n",
      "  [ 1.26867088e+00  8.68263261e+00  1.11421261e+00  3.42388709e+00]]\n",
      "\n",
      " [[ 4.73423028e+00  6.21710000e+00  5.40166024e+00  4.48964323e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 3.21800441e+00  9.95361602e+00  3.78040319e+00  3.54396206e+00]]\n",
      "\n",
      " [[ 5.41943592e+00  6.10462552e+00  5.95407944e+00  7.01900000e+00]\n",
      "  [ 6.01417973e+00  6.88541826e+00  6.17324344e+00  7.91000000e+00]\n",
      "  [ 6.84913557e+00  7.62884235e+00  6.93086221e+00  8.90000000e+00]\n",
      "  [ 7.84919122e+00  8.70090663e+00  7.86740422e+00  1.00000000e+01]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    maze = Maze()\n",
    "    agent = QLearningAgent(maze)\n",
    "    agent.learn()\n",
    "    agent.print_q_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPG650pTDSicGa8x7/7VO+g",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
